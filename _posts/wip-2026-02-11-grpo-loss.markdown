---
layout: post
title:  Deconstructing the GRPO Loss
date:   2026-02-11
tag: technical
---

Group Relative Policy Optimization (GRPO) is a widely used policy gradient algorithm that was popularized by DeepSeek [1].
In this post, we take a look at the loss and discuss how the different components affect model optimization.

For a single query $$q \sim P(Q)$$ and a group of $$G$$ rollouts $$\{o\}_{i=1}^G\sim\pi_\text{old}$$, 
the GRPO loss is defined as:

$$
\begin{align}
\mathcal{J}(\boldsymbol{\theta}) &= \mathbb{E}_{q \sim p(Q), \{o\}_{i=1}^G\sim\pi_\text{old}}\left[
    \frac{1}{G}\sum_{i=1}^G \frac{1}{|o_i|}\sum_{t=1}^{|o_i|} \mathcal{L(\boldsymbol{\theta}, i, t)}
\right] \\
\mathcal{L(\boldsymbol{\theta}, i, t)} &= 
\min(
    r_{i,t}(\boldsymbol{\theta})\hat{A}_{i,t}, 
    \text{clip}(r_{i,t}(\boldsymbol{\theta}), 1 - \varepsilon, 1 + \varepsilon)\hat{A}_{i,t}
) - \beta \mathbb{D}_{\text{KL}}(\pi_\boldsymbol{\theta}(o_{i,t})||\pi_\text{ref}(o_{i,t}))
\end{align}
$$

<br>

#### Understanding $$\mathcal{J}$$
The first line, $$\mathcal{J}(\boldsymbol{\theta})$$ is relatively self-explanatory: we first take a token-level average 
over the loss $$\mathcal{L}$$ then average over the group $$G$$. Note that we only compute the loss on $$o_{i,t}$$, which 
are the tokens generated by the model, and never on the queries $$q$$. The outer expectation is taken with respect to our
query and rollout distributions, which in practice we estimate by averaging over a mini-batch of queries and rollouts.

We should note here that the normalization constant for the token-level average $$1/\vert o_{i,t} \vert$$ can potentially
lead to length bias during optimization. This is becasue $$\vert o_{i,t}\vert$$ will be larger for longer sequences which
can cause shorter sequences to be preferred when $$\hat{A_i} > 0$$ and longer sequences to be preferred when $$\hat{A_i} < 0$$.
Some papers like DAPO [2] mitigate this by summing over the token-level loss, while others like Dr. GRPO [3] take a token-level
average across the full group ($$1 / \sum_{i=1}^G \vert o_{i,t} \vert $$), while others [4] always normalize by the maximum 
possible sequence length. 

In my personal experience training agentic models at Perplexity AI, we did not notice any length bias issues regardless of 
which normalization we used. This is likely because we do not train long chain-of-thought models and instead rely primarily on 
tool use.

<br>

#### Understanding $$\mathcal{L}$$
We now turn to the more interesting part of understanding the token-level loss $$\mathcal{L}$$. If we squint, $$\mathcal{L}$$
decomposes essentially into three parts: $$r_{i,t}(\boldsymbol{\theta})$$, $$\hat{A}_{i, t}$$, and 
$$\mathbb{D}_{\text{KL}}(\pi_\boldsymbol{\theta}(o_{i,t})||\pi_\text{ref}(o_{i,t}))$$. Let us now discuss each in turn.

**Off-policy correction with importance sampling**

The first term is an importance sampling weight and is defined as:

$$
r_{i,t}(\boldsymbol{\theta}) = \frac{\pi_\boldsymbol{\theta}(o_{i,t}|q, o_{i<t})}{\pi_\text{old}(o_{i,t}|q, o_{i<t})}
$$

Its main function is to correct off-policy behavior that arises when rollouts have been generated by a different
policy ($$\pi_\text{old}$$) than the one currently being optimizing ($$\pi_\boldsymbol{\theta}$$). If we naively average 
over the rollouts without accounting for this "off-policyness", we get an incorrect estimate of $$\mathcal{J}(\boldsymbol{\theta})$$ 
and end up optimizing the wrong objective. This can be catastrophic during training and frequently leads to runs 
collapsing, especially when you train for a large number of steps with long sequences. In the case when 
$$\pi_\boldsymbol{\theta} = \pi_\text{old}$$ the importance sampling weights naturally cancel out. 

Off-policy behavior in LLM training almost always arises due to efficiency considerations. The most expensive part of
RL is generating rollouts so modern RL systems use specialized inference engines like vLLM [5] or SGLang [6]. These systems make 
use of specialized kernels, quantization, lower precision computation, etc, to maximize generation throughput. This will almost
certainly cause divergecnes between the training system ($$\pi_\boldsymbol{\theta}$$) and the inference system ($$\pi_\text{old}$$)
necessitating either off-policy correction [7] or significant infrastructure work to ensure the training and inference systems are
bitwise identical [8].

Since rollouts take up a significant portion of each RL step, another common practice is to rollout many steps in parallel and 
mini-batch the rollouts. Even when training and inference are bitwise identical, this approach will go off-policy after the first
gradient step and will require off-policy correction thereafter. More recently async-RL approaches like PipelineRL [9] have been proposed 
to further improve rollout speed and throughput. These methods completely decouple training and inference and may generate long rollouts
using multiple policies.

*Clipping.*

*Minimizing.*


**Estimating advantages as group-relative rewards**

TODO


**Staying close to the base model**

TODO